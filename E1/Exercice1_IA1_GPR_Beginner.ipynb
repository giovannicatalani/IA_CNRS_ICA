{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycFusWaBfCjy"
   },
   "source": [
    "**IA@ICA by Professor J. Morlier**\n",
    "\n",
    "From the famous example of \n",
    "Ebden, M. (2008). Gaussian processes for regression: A quick introduction. The website of robotics research group in department on engineering science, University of Oxford, 91, 424-436."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbC63MYAZwPz"
   },
   "source": [
    "**EXERCICE 1:** Given some noisy observations of a dependent variable at certain values of the independent variable , what is our best estimate of the dependent variable at a new value x* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "oqXUG15dX_3V",
    "outputId": "b382ebcf-9f60-4191-997a-4b3b45c0abb2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Inputs\n",
    "X = np.array([-1.5, -1, -0.75, -0.4, -0.25, 0]).reshape(-1, 1)\n",
    "n = X.shape[0]\n",
    "\n",
    "# Outputs\n",
    "y = np.array([-1.6, -1.10, -.4, .2, 0.6, 1]).reshape(-1, 1)\n",
    "\n",
    "# y = 0.55 * np.array([-3, -2, -0.6, 0.4, 1, 1.6]).reshape(-1, 1)\n",
    "\n",
    "# Plotting data\n",
    "plt.figure()\n",
    "plt.plot(X, y, 'k.')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DD22Mom2cBnZ"
   },
   "source": [
    "What if we want to estimate the model at a new point  x* defined as 18 points in [-1.5,0]. Please check also x*= 0.2 (extrapolation) !!\n",
    "The regression problem is defined as follows:\n",
    "Let $\\mathbf{x}_i \\in {R}^{6}$ be an input vector and $\\mathbf{y}_i \\in {R}^{6}$ be its corresponding target. The set of $M$ inputs are arranged into a matrix $\\mathbf{X} = [\\mathbf{x}_1, \\dots, \\mathbf{x}_M]^\\top$ and their corresponding targets are stored in a matrix $\\mathbf{Y} = [\\mathbf{y}_1 - \\mathbf{\\bar{y}}, \\dots, \\mathbf{y}_M-\\mathbf{\\bar{y}}]^\\top$, with $\\mathbf{\\bar{y}}$  being the mean target value in $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QF5c08czYaA0"
   },
   "outputs": [],
   "source": [
    "# Interpolation and extrapolation points\n",
    "# go up to xstar = 0.2 # Extrapolation\n",
    "# N=1\n",
    "xstar = np.arange(-1.5, 0.21, 0.1).reshape(-1, 1) # 18 points in [-1.5,0] interpolation\n",
    "N = xstar.shape[0]\n",
    "\n",
    "# Computing covariance in matrix form\n",
    "covXXInd1, covXXInd2 = np.meshgrid(X, X)\n",
    "covXXsInd1, covXXsInd2 = np.meshgrid(X, xstar)\n",
    "covXsXsInd1, covXsXsInd2 = np.meshgrid(xstar, xstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ6cX0F-cvIn"
   },
   "source": [
    "hey hey add something missing, we may need parameters to fit \"a model\" of our covariance matrix. Let's try with a Standard Exponential (SE) Kernel $k(x,x') =\\sigma_f^2\\exp\\left(-\\frac{(x-x')^2}{l^ 2}\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8E7D-PEtcufO"
   },
   "outputs": [],
   "source": [
    "# Judicious Hyperparameters\n",
    "l = 1\n",
    "sig_f = np.sqrt(3)\n",
    "\n",
    "# Computing covariance matrices through SE Kernel\n",
    "covXX = ?\n",
    "covXsXs = ?\n",
    "covXXs = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wish to train a GPR model $\\mathcal{G} = \\lbrace \\mathbf{X}, \\mathbf{Y}, \\theta \\rbrace$ using the squared exponential function ( $\\theta$ must be chosen):\n",
    "$k(\\mathbf{x}_i, \\mathbf{x}_j) = \\sigma_f^2 \\text{exp} \\left( - \\frac{1}{l^2}(\\mathbf{x}_i - \\mathbf{x}_j)^2\\right) + \\sigma_n^2\\delta_{ij}$\n",
    ", where $\\delta_{ij}$ equals 1 if $i = j$ and 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mcje9S1OeB36"
   },
   "outputs": [],
   "source": [
    "# Adding noise to covariance matrix, let's try less?\n",
    "sig_n = 0.3\n",
    "covXX_noisy = covXX + sig_n ** 2 * np.eye(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "44RynqymeGY9",
    "outputId": "85089f8e-aebd-49e5-e15e-da2bb0b5489b"
   },
   "outputs": [],
   "source": [
    "# Plotting covariance matrix with and without noise\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the first image on the left subplot\n",
    "im0 = axes[0].imshow(covXX_noisy, cmap='viridis')  # You can choose a colormap that suits your data\n",
    "axes[0].set_title('Noisy Covariance Matrix')\n",
    "axes[0].set_axis_off()  # Optional: Turn off axis labels and ticks\n",
    "\n",
    "# Plot the second image on the right subplot\n",
    "im1 = axes[1].imshow(covXX, cmap='viridis')  # You can choose a colormap that suits your data\n",
    "axes[1].set_title('Non Noisy Covariance Matrix')\n",
    "axes[1].set_axis_off()  # Optional: Turn off axis labels and ticks\n",
    "\n",
    "# Display the colorbars if needed\n",
    "cbar0 = fig.colorbar(im0, ax=axes[0])\n",
    "cbar1 = fig.colorbar(im1, ax=axes[1])\n",
    "\n",
    "# Adjust layout to prevent clipping of titles\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWXxarVueG0Y"
   },
   "outputs": [],
   "source": [
    "# Computing posterior mean and covariance\n",
    "\n",
    "def posterior(covXXs,covXX,covXsXs):\n",
    "    posterior_mean = ?\n",
    "    posterior_cov  = ?\n",
    "    return posterior_mean,posterior_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "xoDmsXIcaD12",
    "outputId": "e8a30152-04d0-4689-f3c7-a0246fa9c7a1"
   },
   "outputs": [],
   "source": [
    "# Computing posterior mean and covariance without noise\n",
    "posterior_mean, posterior_cov  = posterior(covXXs,covXX,covXsXs)\n",
    "\n",
    "# Computing posterior mean and covariance with noise\n",
    "posterior_mean_noisy, posterior_cov_noisy  = posterior(covXXs,covXX_noisy,covXsXs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the mean and variance of the posterior without noise\n",
    "upper_bound = posterior_mean.flatten() + 2 * np.sqrt(np.diag(posterior_cov))\n",
    "lower_bound = posterior_mean.flatten() - 2 * np.sqrt(np.diag(posterior_cov))\n",
    "\n",
    "axes[0].fill_between(xstar.flatten(), upper_bound, lower_bound, color=[7/8, 7/8, 7/8])\n",
    "axes[0].plot(xstar, posterior_mean, 'b-', linewidth=2)\n",
    "axes[0].plot(X, y, 'k.')\n",
    "axes[0].set_title('Without noise')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "\n",
    "\n",
    "# Plot the mean and variance of the posterior with noise\n",
    "upper_bound_noisy = posterior_mean_noisy.flatten() + 2 * np.sqrt(np.diag(posterior_cov_noisy))\n",
    "lower_bound_noisy = posterior_mean_noisy.flatten() - 2 * np.sqrt(np.diag(posterior_cov_noisy))\n",
    "\n",
    "axes[1].fill_between(xstar.flatten(), upper_bound_noisy, lower_bound_noisy, color=[7/8, 7/8, 7/8])\n",
    "axes[1].plot(xstar, posterior_mean_noisy, 'b-', linewidth=2)\n",
    "axes[1].plot(X, y, 'k.')\n",
    "axes[1].set_title('With noise')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('y')\n",
    "\n",
    "\n",
    "axes[1].set_ylim([min(lower_bound_noisy), max(upper_bound_noisy)])  # Set y-axis limits\n",
    "axes[0].set_ylim([min(lower_bound_noisy), max(upper_bound_noisy)])  # Set y-axis limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "FMSkXfO6YhaE",
    "outputId": "ade18a0c-9b2c-456d-9112-d5d76a6b67d3"
   },
   "outputs": [],
   "source": [
    "# Generating random function samples from posterior\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "jitter = 10 ** (-6)\n",
    "\n",
    "# Without noise\n",
    "\n",
    "axes[0].plot(xstar, posterior_mean,'.--')\n",
    "axes[0].set_title('Without noise')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "\n",
    "\n",
    "# Wiht noise\n",
    "L = np.linalg.cholesky(posterior_cov_noisy + jitter * np.eye(N))\n",
    "random_functions = posterior_mean_noisy + L.T @ np.random.randn(N, 5)\n",
    "\n",
    "axes[1].fill_between(xstar.flatten(), upper_bound_noisy, lower_bound_noisy, color=[7/8, 7/8, 7/8])\n",
    "axes[1].plot(xstar, random_functions,'.--')\n",
    "axes[1].set_title('With noise')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2QWxp5Sd21B"
   },
   "source": [
    "but is there a way to find the $\\theta_{optimal}$ ? oh $\\theta = [l, \\sigma_f, \\sigma_n]$ \n",
    "\n",
    "The hyperparameters are $\\theta = [l, \\sigma_f, \\sigma_n]$  with $\\sigma_n$ being the assumed noise level in the training data and $l$ is the length-scale  (of oscillations) and $\\sigma_f$ the amplitude.\n",
    "To train the model, I need to minimise the negative log marginal likelihood with respect to the hyperparameters:\n",
    "$-\\text{log}\\, p(\\mathbf{Y} \\mid \\mathbf{X}, \\theta) = \\frac{1}{2} \\text{tr}(\\mathbf{Y}^\\top\\mathbf{K}^{-1}\\mathbf{Y}) + \\frac{1}{2}\\text{log}\\mid\\mathbf{K}\\mid + \\,c,$\n",
    "where c is a constant and the matrix $\\mathbf{K}$ is a function of the hyperparameters (see equation k(xi,xj) = ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBN5qViYfBTr"
   },
   "source": [
    "For this purpose. Let's move to **EXERCICE 2** and use scikit learn and SMT"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
